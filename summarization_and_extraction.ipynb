{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb15ff63-4e46-4f81-b8ba-d18e3b91e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from openai==0.28) (4.67.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from openai==0.28) (3.11.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.9/site-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.4\n",
      "    Uninstalling openai-1.54.4:\n",
      "      Successfully uninstalled openai-1.54.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogen 0.3.2 requires openai>=1.3, but you have openai 0.28.0 which is incompatible.\n",
      "pyautogen 0.3.2 requires openai>=1.3, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2b306-e024-45e3-8544-c04c66719067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/naren/.local/lib/python3.9/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb04e6b2-e6ff-4002-b832-d20f0dc9e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/naren/.local/lib/python3.9/site-packages (4.3.3)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/site-packages (3.3.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.67.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/naren/.local/lib/python3.9/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim sentence-transformers nltk scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8584c680-8737-42b4-8ad5-16ce9c518b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a971832-6be7-4cf3-bf6f-f14c338da54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(openai.__version__)\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015a0615-9fa9-49cb-850f-c08e074f3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript(file_path):\n",
    "    \"\"\"Load the conversation transcript from a text file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664a5fe6-9480-491c-b3d0-d0f1309db63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_duration(transcript_text):\n",
    "    \"\"\"Estimate conversation duration based on transcript word count.\"\"\"\n",
    "    words = len(transcript_text.split())\n",
    "    minutes = words // 150  # Average speaking speed is 150 WPM\n",
    "    seconds = (words % 150) * 60 // 150  # Remaining words converted to seconds\n",
    "    return f\"{minutes:02}:{seconds:02}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f8ed22-5803-4eee-a95b-ec1971e83794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_transcript(transcript_text):\n",
    "    \"\"\"Generate a concise summary using GPT-4o-mini with generalized prompt engineering.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an intelligent summarization assistant for conversations.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Please summarize the following conversation transcript:\\n\\n{transcript_text}\\n\\n\"\n",
    "                    \"Provide the summary in JSON format with the following structure:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  'summary': 'One or two sentences that provide an overall summary of the conversation.',\\n\"\n",
    "                    \"  'duration': 'MM:SS',\\n\"\n",
    "                    \"  'participant_count': [number of unique participants],\\n\"\n",
    "                    \"  'main_points': [\\n\"\n",
    "                    \"    '- Describe any roles or responsibilities mentioned by participants.',\\n\"\n",
    "                    \"    '- Summarize the main topics of discussion in the conversation.',\\n\"\n",
    "                    \"    '- Note any strong opinions, preferences, or decisions shared by participants.'\\n\"\n",
    "                    \"  ]\\n\"\n",
    "                    \"}\\n\"\n",
    "                    \"Ensure that the main points are flexible and relevant to any type of conversation transcript. \"\n",
    "                    \"Make sure to keep the points concise and directly related to the conversation content.\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.4\n",
    "    )\n",
    "    summary_data = response.choices[0].message['content'].strip()\n",
    "    return summary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca59031-f0c2-437d-a1d1-b4015bfe37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summary(summary_text, transcript_text):\n",
    "    \"\"\"Extract key details from the summary and format them as JSON.\"\"\"\n",
    "    \n",
    "    # Calculate duration dynamically\n",
    "    duration = calculate_duration(transcript_text)\n",
    "    \n",
    "    # Dynamically count unique participants based on conversation markers (e.g., \"M:\" or \"R:\" for speakers)\n",
    "    participant_count = len(set(re.findall(r'^[A-Z]:', transcript_text, re.MULTILINE)))\n",
    "\n",
    "    # Extract main points in a more general way\n",
    "    main_points_raw = re.findall(r\"- (.+)\", summary_text)\n",
    "    main_points = [re.sub(r'[\\\\\",]+$', '', point.strip()) for point in main_points_raw]  # Clean up any trailing characters\n",
    "\n",
    "    # Ensure we always have three main points, even if we need to add generic placeholders\n",
    "    while len(main_points) < 3:\n",
    "        main_points.append(\"Point not provided by model.\")\n",
    "\n",
    "    json_output = {\n",
    "        \"summary\": \"A comprehensive overview of the conversation\",\n",
    "        \"duration\": duration,\n",
    "        \"participant_count\": participant_count,\n",
    "        \"main_points\": main_points[:3]  # Limit to 3 main points as requested\n",
    "    }\n",
    "    return json_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d74d31-c3fa-4c18-ae5d-a1ff88d99c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path):\n",
    "    # Load transcript\n",
    "    transcript_text = load_transcript(file_path)\n",
    "    \n",
    "    # Get summary from GPT-4o-mini\n",
    "    summary_text = summarize_transcript(transcript_text)\n",
    "    \n",
    "    # Process and format the summary into JSON\n",
    "    json_output = process_summary(summary_text, transcript_text)\n",
    "    \n",
    "    # Print the JSON output\n",
    "    print(json.dumps(json_output, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc284e25-f3ff-4ef6-9eed-70a68414098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"A comprehensive overview of the conversation\",\n",
      "  \"duration\": \"61:42\",\n",
      "  \"participant_count\": 2,\n",
      "  \"main_points\": [\n",
      "    \"Renuka is responsible for household decisions, including cooking and health management for her family.\",\n",
      "    \"The discussion covers Renuka's daily routine, her family's health concerns, and her approach to purchasing medications.\",\n",
      "    \"Renuka prefers to consult doctors for medication and expresses skepticism about generic medicines, emphasizing the importance of doctor recommendations.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\"Round2-Assessment-transcript.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46f7521-c5be-4f7b-a2bb-ff9b1b18ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/site-packages (3.3.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.67.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scipy in /Users/naren/.local/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/naren/.local/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers nltk scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8afa5be0-6d92-4f51-9925-f7d5719954ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 21:08:04.313606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to /Users/naren/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/naren/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Load pre-trained Sentence Transformer model for semantic embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight and efficient\n",
    "\n",
    "# Ensure stopwords are downloaded (if not already)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cabc07c-e89e-4e49-b297-c1607fe87025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses the input text, removing stopwords and short tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to preprocess.\n",
    "        stop_words (set): A set of stopwords to be removed from the text.\n",
    "    Returns:\n",
    "        list: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, deacc=True):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a0d5b0-e8ce-484a-8b45-2f5683abfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_keywords(transcript, num_topics=10, words_per_topic=5):\n",
    "    \"\"\"\n",
    "    Extracts topics and their associated keywords from the transcript using LDA.\n",
    "\n",
    "    Parameters:\n",
    "        transcript (str): The transcript text to process.\n",
    "        num_topics (int): The number of topics to discover.\n",
    "        words_per_topic (int): The number of words to include per topic.\n",
    "\n",
    "    Returns:\n",
    "        List: A list of dictionaries, each containing a topic name and keywords.\n",
    "    \"\"\"\n",
    "    # Tokenize the transcript into sentences for LDA processing\n",
    "    sentences = sent_tokenize(transcript)\n",
    "    \n",
    "    # Preprocess sentences\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_documents = [preprocess(sentence, stop_words) for sentence in sentences]\n",
    "    \n",
    "    # Create a dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(processed_documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "    \n",
    "    # Build the LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=0)\n",
    "    \n",
    "    # Retrieve topics with keywords\n",
    "    topics = []\n",
    "    for idx, topic in lda_model.print_topics(num_words=words_per_topic):\n",
    "        words = [word.split(\"*\")[1].replace('\"', '').strip() for word in topic.split(\" + \")]\n",
    "        topics.append({\"topic_id\": idx, \"keywords\": words})\n",
    "    \n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41b4030d-8d2a-4ab9-b0de-248ff24c587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_task_outputs(task1_output, task2_detailed_topics):\n",
    "    \"\"\"\n",
    "    Combines the high-level summary from Task 1 with the detailed topics from Task 2.\n",
    "    \n",
    "    Parameters:\n",
    "        task1_output (dict): The output JSON from Task 1 containing the summary.\n",
    "        task2_detailed_topics (dict): The detailed topics JSON output from Task 2.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Combined JSON output.\n",
    "    \"\"\"\n",
    "    combined_output = task1_output\n",
    "    combined_output[\"detailed_topics\"] = task2_detailed_topics[\"detailed_topics\"]\n",
    "    return combined_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39980ea2-5756-409f-8297-1b91c6ec9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_summaries(topics, num_topics):\n",
    "    \"\"\"\n",
    "    Generates summaries for each topic using an LLM chat model based on topic keywords.\n",
    "\n",
    "    Parameters:\n",
    "        topics (list): A list of topics with associated keywords.\n",
    "        num_topics (int): The number of topics to summarize.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of summaries with detailed descriptions and subtopics.\n",
    "    \"\"\"\n",
    "    prompt_template = '''\n",
    "For each of the {num_topics} topics, provide a relevant and concise topic description and identify 3 key subtopics. The descriptions and subtopics should be closely connected to the keywords provided.\n",
    "\n",
    "Format the response like this:\n",
    "\n",
    "1: Topic Description\n",
    "- Subtopic 1\n",
    "- Subtopic 2\n",
    "- Subtopic 3\n",
    "\n",
    "Topics: {topics_keywords}\n",
    "'''\n",
    "\n",
    "    # Prepare the keywords into a format for the LLM\n",
    "    topics_keywords = \"\\n\".join([f\"{i+1}: {', '.join(topic['keywords'])}\" for i, topic in enumerate(topics[:num_topics])])\n",
    "    prompt = prompt_template.format(num_topics=num_topics, topics_keywords=topics_keywords)\n",
    "    \n",
    "    # Call the OpenAI Chat API to generate descriptions\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  # Use the correct chat model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an intelligent assistant providing topic summaries with subtopics.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=400,\n",
    "        temperature=0.4\n",
    "    )\n",
    "    \n",
    "    # Parse the LLM's response\n",
    "    topic_summaries = response.choices[0].message['content'].strip().split(\"\\n\\n\")\n",
    "    structured_summaries = []\n",
    "    for summary in topic_summaries:\n",
    "        lines = summary.strip().split(\"\\n\")\n",
    "        if len(lines) < 4:\n",
    "            continue\n",
    "        topic_description = lines[0][3:].strip()  # Remove \"1: \" prefix\n",
    "        subtopics = [line[2:].strip() for line in lines[1:4]]  # Remove \"- \" prefix from subtopics\n",
    "        structured_summaries.append({\n",
    "            \"topic_name\": topic_description,\n",
    "            \"subtopics\": subtopics\n",
    "        })\n",
    "    \n",
    "    return structured_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ceb6fd0-bff8-4ec5-a3b9-ca54668642d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output_for_task2(transcript, topic_summaries, num_topics=10):\n",
    "    \"\"\"\n",
    "    Combines the generated topic summaries with the transcript data in Task 2's required format.\n",
    "\n",
    "    Parameters:\n",
    "        transcript (str): The original transcript text.\n",
    "        topic_summaries (list): A list of summaries generated by the LLM.\n",
    "\n",
    "    Returns:\n",
    "        dict: The output in the JSON format specified for Task 2.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(transcript)\n",
    "    output = []\n",
    "\n",
    "    # Embed each sentence for better matching\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    for i, summary in enumerate(topic_summaries[:num_topics]):\n",
    "        topic_name = summary[\"topic_name\"]\n",
    "        subtopics = summary[\"subtopics\"]\n",
    "        \n",
    "        # Get topic embedding for similarity matching\n",
    "        topic_embedding = model.encode([topic_name])[0]\n",
    "        \n",
    "        # Calculate similarity between topic and each sentence\n",
    "        similarities = cosine_similarity([topic_embedding], sentence_embeddings).flatten()\n",
    "        \n",
    "        # Select top N relevant sentences based on similarity\n",
    "        top_indices = similarities.argsort()[-5:][::-1]  # Top 5 similar sentences for each topic\n",
    "        relevant_sentences = [sentences[idx] for idx in top_indices]\n",
    "\n",
    "        # Prepare structured data for the topic\n",
    "        topic_data = {\n",
    "            \"topic_name\": topic_name,\n",
    "            \"span\": f\"{top_indices[0]} - {top_indices[-1]}\",  # approximate span based on top sentences\n",
    "            \"relevant_excerpts\": relevant_sentences,\n",
    "            \"subtopics\": subtopics\n",
    "        }\n",
    "        output.append(topic_data)\n",
    "    \n",
    "    return {\"detailed_topics\": output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095b5c63-4d9f-4810-83d8-715f3eecef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(file_path, num_topics=7, words_per_topic=5):\n",
    "    \"\"\"\n",
    "    Process the transcript to extract topics, generate summaries, and structure the output.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        transcript = file.read()\n",
    "    \n",
    "    # Step 1: Extract topics using LDA or other topic modeling technique\n",
    "    topics = get_topic_keywords(transcript, num_topics=num_topics, words_per_topic=words_per_topic)\n",
    "    \n",
    "    # Step 2: Generate topic summaries with LLM\n",
    "    topic_summaries = generate_topic_summaries(topics, num_topics)\n",
    "    \n",
    "    # Step 3: Format the output as specified\n",
    "    detailed_topics = format_output_for_task2(transcript, topic_summaries, num_topics)\n",
    "    \n",
    "    # Print the final JSON output\n",
    "    print(json.dumps(detailed_topics, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b245431-62e9-40d3-b036-d436827a5ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"detailed_topics\": [\n",
      "    {\n",
      "      \"topic_name\": \"The Evolution of Tablets Over the Years\",\n",
      "      \"span\": \"520 - 494\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"Are you talking about tablets?\",\n",
      "        \"I have seen those tablets, but never thought to buy them.\",\n",
      "        \"So from there, I got to know that they are government-based tablets.\",\n",
      "        \"When it comes to the turn of other tablets like Dolo, crocin etc, when do you get them?\",\n",
      "        \"R: We are aware of the name of the tablet and its details, right?\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"blets have transformed from simple devices to powerful tools for communication, education, and entertainment. This topic explores their development and impact over time.\",\n",
      "        \"Historical milestones in tablet technology\",\n",
      "        \"Major brands and their contributions\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"Understanding People\\u2019s Preferences for Tablets\",\n",
      "      \"span\": \"539 - 544\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"But people don't know that they are low-cost, and also they don't have any idea whether those tablets are powerful or if they work.\",\n",
      "        \"Are you talking about tablets?\",\n",
      "        \"I have seen those tablets, but never thought to buy them.\",\n",
      "        \"R: We are aware of the name of the tablet and its details, right?\",\n",
      "        \"So from there, I got to know that they are government-based tablets.\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"is topic delves into how people perceive and choose tablets based on their needs and preferences.\",\n",
      "        \"Factors influencing tablet selection\",\n",
      "        \"Popular tablet features among consumers\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"The Benefits of Buying Good Quality Medicines at a Discount\",\n",
      "      \"span\": \"486 - 609\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"So, in terms of money, we can save a little at Net Meds while still getting the same brand and genuine medicines.\",\n",
      "        \"Therefore, for profit and pricing margin purposes, medical shop owners keep and sell different medicines.\",\n",
      "        \"They advise the shop owners to stock alternative medicines and sell them.\",\n",
      "        \"R: In 100rs, if they are giving a 70% or 60% discount, then we will have doubts because the value of the medicine might have decreased.\",\n",
      "        \"Do you think that is positive or negative, generic medicines being available at low cost?\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"ploring the importance of purchasing high-quality medications while also seeking discounts, this topic emphasizes health savings.\",\n",
      "        \"Strategies for finding discounts on medicines\",\n",
      "        \"The impact of quality on health outcomes\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"The Role of Generic Medicines in Healthcare\",\n",
      "      \"span\": \"555 - 551\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"M: What else do you know about generic medicines?\",\n",
      "        \"M: What is your personal opinion on generic medicines?\",\n",
      "        \"Have you ever heard of generic medicines?\",\n",
      "        \"M: How did you get to know about generic medicines?\",\n",
      "        \"M: Even though you don't have much idea about them, what are your thoughts on generic medicines?\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"neric medicines provide affordable alternatives to brand-name drugs, making healthcare more accessible. This topic examines their significance.\",\n",
      "        \"Differences between generic and brand-name medicines\",\n",
      "        \"Regulatory processes for approving generics\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"Consumer Perspectives on Shopping for Medicines\",\n",
      "      \"span\": \"530 - 470\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"Many sales representatives come to medical shops to sell their medicines.\",\n",
      "        \"They advise the shop owners to stock alternative medicines and sell them.\",\n",
      "        \"Therefore, for profit and pricing margin purposes, medical shop owners keep and sell different medicines.\",\n",
      "        \"They have also arranged the medicines properly and neatly, making it look attractive and easy to identify as a medical store.\",\n",
      "        \"Still, most of the time, it's me who goes to buy medicines.\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"is topic investigates how consumers approach the purchase of medicines, including their experiences and thoughts on various options.\",\n",
      "        \"Common concerns when buying medicines\",\n",
      "        \"The influence of online shopping on medicine purchases\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"The Importance of Consulting a Doctor for Smart Medicine Choices\",\n",
      "      \"span\": \"188 - 729\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"Few other doctors are also available here.\",\n",
      "        \"If doctors...\\nM: So with TV ads and doctor suggestions, you believe it.\",\n",
      "        \"Whether it\\u2019s tablets or medicines, for example, there is a brand called \\u2018Dr.\",\n",
      "        \"R: For that, we have to depend on doctors.\",\n",
      "        \"Who can provide those medicines better?\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"is topic highlights the value of professional medical advice in making informed decisions about medications.\",\n",
      "        \"How doctors can help optimize medication use\",\n",
      "        \"The relationship between doctor consultations and health savings\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"topic_name\": \"Apollo Hospitals and the Rise of Generic Medicines\",\n",
      "      \"span\": \"786 - 782\",\n",
      "      \"relevant_excerpts\": [\n",
      "        \"R: As per my knowledge, Apollo people are hoping to bring generic medicines to people.\",\n",
      "        \"So, in the beginning, \\u2018on Apollo generics\\u2019 should come first, then in the second, \\u2018for more information, contact our pharmacist\\u2019 should come.\",\n",
      "        \"It is about Apollo generics we are consulting, right?\",\n",
      "        \"R: They are saying about generics, and through this, we can know that we can approach them about this and we can approach Apollo pharmacist.\",\n",
      "        \"So, writing \\u2018on Apollo generics\\u2019 matters first, then \\u2018for more information, talk to our pharmacist\\u2019 matters, and then in the caption \\u2018same quality, plus better price, smart savings\\u2019 would be good.\"\n",
      "      ],\n",
      "      \"subtopics\": [\n",
      "        \"cusing on Apollo Hospitals, this topic discusses their role in promoting generic medicines and improving healthcare access.\",\n",
      "        \"Apollo's initiatives for affordable healthcare\",\n",
      "        \"The impact of generics on patient\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Round2-Assessment-transcript.txt\"\n",
    "task2(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac3d6c-bb97-4201-8fee-480d33be6b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
